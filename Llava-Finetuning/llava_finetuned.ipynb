{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a4acc420-4e7e-4557-b205-f1e2d9f4c56c",
      "metadata": {
        "id": "a4acc420-4e7e-4557-b205-f1e2d9f4c56c"
      },
      "source": [
        "\n",
        "\n",
        "# Fine-tune and deploy the multimodal LLaVA model with DeepSpeed\n",
        "\n",
        "Hi everyone!\n",
        "\n",
        "In this notebook we'll fine-tune the LLaVA model. LLaVA is multimodal which means it can ingest and understand images along with text! LLaVA comes from a research paper titled [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) and introduces the Large Language and Vision Assistant methodology. In order to process images, LLaVA relies on the pre-trained CLIP visual encoder ViT-L/14 which maps images and text into the same latent space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prerequisites\n",
        "- High-end GPUs such as NVIDIA A100 or V100 are recommended for efficient training.\n",
        "- Ensure sufficient memory (40-80GB) and storage for datasets and model checkpoints."
      ],
      "metadata": {
        "id": "jQDvMqsSJlBJ"
      },
      "id": "jQDvMqsSJlBJ"
    },
    {
      "cell_type": "markdown",
      "id": "c5808bde-44da-4588-95f4-68c292cf9400",
      "metadata": {
        "id": "c5808bde-44da-4588-95f4-68c292cf9400"
      },
      "source": [
        "## Table of contents\n",
        "\n",
        "1. Data Preprocessing\n",
        "2. LLaVA Installation\n",
        "3. DeepSpeed configuration\n",
        "4. Weights and Biases\n",
        "5. Finetuning flow\n",
        "6. Deployment via gradio interface"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b2638c4-fea4-451d-a512-4e2c731cbdec",
      "metadata": {
        "id": "5b2638c4-fea4-451d-a512-4e2c731cbdec"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "LLaVA requires data to be in a very specific format. Below we use a [helper function](https://wandb.ai/byyoung3/ml-news/reports/How-to-Fine-Tune-LLaVA-on-a-Custom-Dataset--Vmlldzo2NjUwNTc1) to format the OKV-QA dataset. This dataset teaches the model to respond to an image in short phrases without any preamble or extra verbiage."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2939e285-2a00-4b6c-bfb0-c0fa09d0fe7a",
      "metadata": {
        "id": "2939e285-2a00-4b6c-bfb0-c0fa09d0fe7a"
      },
      "source": [
        "### Optional: Create your own dataset using GPT-4o\n",
        "\n",
        "The guide to creating your own dataset is relatively simple! Here's a simple script that you could use that leverages GPT4o's multimodal capabilities to quickly create a dataset that can be used in the dataset creator function that we write below!\n",
        "\n",
        "\n",
        "```python\n",
        "import os\n",
        "import base64\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# OpenAI API Key\n",
        "api_key = \"<enter key here>\"\n",
        "\n",
        "# Function to encode the image\n",
        "def encode_image(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "# Path to your images folder\n",
        "folder_path = \"<enter image folder path here>\"\n",
        "\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {api_key}\"\n",
        "}\n",
        "\n",
        "# Question to ask for each image\n",
        "question = \"Generate a detailed description about this image\" #change this depending on your use case\n",
        "\n",
        "# Function to process each image and get the description\n",
        "def process_image(image_path, image_name):\n",
        "    base64_image = encode_image(image_path)\n",
        "    \n",
        "    payload = {\n",
        "        \"model\": \"gpt-4o\",\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": question\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\n",
        "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "                        }\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        \"max_tokens\": 300\n",
        "    }\n",
        "\n",
        "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
        "    response_json = response.json()\n",
        "    return response_json['choices'][0]['message']['content']\n",
        "\n",
        "# List to store all JSON data\n",
        "all_json_data = []\n",
        "\n",
        "# Process each image in the folder\n",
        "for image_name in os.listdir(folder_path):\n",
        "    if image_name.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "        image_path = os.path.join(folder_path, image_name)\n",
        "        formatted_answers = process_image(image_path, image_name)\n",
        "        \n",
        "        json_data = {\n",
        "            \"id\": image_name.split('.')[0],\n",
        "            \"image\": image_name,\n",
        "            \"conversations\": [\n",
        "                {\n",
        "                    \"from\": \"human\",\n",
        "                    \"value\": question\n",
        "                },\n",
        "                {\n",
        "                    \"from\": \"gpt\",\n",
        "                    \"value\": formatted_answers\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "        \n",
        "        all_json_data.append(json_data)\n",
        "\n",
        "# Save the results to a JSON file\n",
        "output_file = \"output.json\"\n",
        "with open(output_file, \"w\") as outfile:\n",
        "    json.dump(all_json_data, outfile, indent=4)\n",
        "\n",
        "print(f\"Data has been saved to {output_file}\")\n",
        "\n",
        "```\n",
        "How to use this script\n",
        "1. Create a folder called dataset. Inside of this folder, create a subfolder called images.\n",
        "2. Place all your images in a directory and specify that path as folder_path.\n",
        "3. Outputs are saved in a JSON file in the specified output_folder, pairing each image file with its generated description.\n",
        "4. After the script is run, create another folder inside dataset called train and move the output.json file into this folder."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc3d7b07-4e3d-4255-a940-7b5820b100e1",
      "metadata": {
        "id": "dc3d7b07-4e3d-4255-a940-7b5820b100e1"
      },
      "source": [
        "## Back to Finetuning!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4cf7a70-0294-4a75-bffe-d0c375ce11ff",
      "metadata": {
        "scrolled": true,
        "id": "b4cf7a70-0294-4a75-bffe-d0c375ce11ff"
      },
      "outputs": [],
      "source": [
        "# Install preprocessing libraries\n",
        "!pip install datasets\n",
        "!pip install --upgrade --force-reinstall Pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7b98b3d-6781-4abc-b95f-e58b274afcd8",
      "metadata": {
        "id": "c7b98b3d-6781-4abc-b95f-e58b274afcd8"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import requests\n",
        "import os\n",
        "import json\n",
        "import uuid\n",
        "\n",
        "# Check PIL import\n",
        "import PIL.Image\n",
        "\n",
        "# Define preprocessing functions\n",
        "def process_and_save(dataset, output_folder, subset_name):\n",
        "    # Define image subfolder within output folder\n",
        "    subset_folder = os.path.join(output_folder, subset_name)\n",
        "    image_subfolder = os.path.join(output_folder, 'images')\n",
        "\n",
        "    if not os.path.exists(image_subfolder):\n",
        "        os.makedirs(image_subfolder)\n",
        "\n",
        "    if not os.path.exists(subset_folder):\n",
        "        os.makedirs(subset_folder)\n",
        "\n",
        "    # Initialize list to hold all JSON data\n",
        "    json_data_list = []\n",
        "\n",
        "    # Process and save images and labels\n",
        "    for item in dataset:\n",
        "        # Load image if it's a URL or a file path\n",
        "        if isinstance(item['image'], str):\n",
        "            response = requests.get(item['image'])\n",
        "            image = Image.open(BytesIO(response.content))\n",
        "        else:\n",
        "            image = item['image']  # Assuming it's a PIL.Image object\n",
        "\n",
        "        # Create a unique ID for each image\n",
        "        unique_id = str(uuid.uuid4())\n",
        "\n",
        "        # Define image path\n",
        "        image_path = os.path.join(image_subfolder, f\"{unique_id}.jpg\")\n",
        "\n",
        "        # Save image\n",
        "        image.save(image_path)\n",
        "\n",
        "        # Remove duplicates and format answers\n",
        "        answers = item['answers']\n",
        "        unique_answers = list(set(answers))\n",
        "        formatted_answers = \", \".join(unique_answers)\n",
        "\n",
        "        # Structure for LLaVA JSON\n",
        "        json_data = {\n",
        "            \"id\": unique_id,\n",
        "            \"image\": f\"{unique_id}.jpg\",\n",
        "            \"conversations\": [\n",
        "                {\n",
        "                    \"from\": \"human\",\n",
        "                    \"value\": item['question']\n",
        "                },\n",
        "                {\n",
        "                    \"from\": \"gpt\",\n",
        "                    \"value\": formatted_answers\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # Append to list\n",
        "        json_data_list.append(json_data)\n",
        "\n",
        "    # Save the JSON data list to a file\n",
        "    json_output_path = os.path.join(output_folder, subset_name, 'dataset.json')\n",
        "    with open(json_output_path, 'w') as json_file:\n",
        "        json.dump(json_data_list, json_file, indent=4)\n",
        "\n",
        "def save_dataset(dataset_name, output_folder, class_name, subset_name, val_samples=None):\n",
        "    # Load the dataset from Hugging Face\n",
        "    dataset = load_dataset(dataset_name, split=subset_name)\n",
        "\n",
        "    # Filter for images with the specified class in 'question_type'\n",
        "    filtered_dataset = [item for item in dataset if item['question_type'] == class_name]\n",
        "\n",
        "    # Determine the split for training and validation\n",
        "    if val_samples is not None and subset_name == 'train':\n",
        "        train_dataset = filtered_dataset[val_samples:]\n",
        "        val_dataset = filtered_dataset[:val_samples]\n",
        "    else:\n",
        "        train_dataset = filtered_dataset\n",
        "        val_dataset = []\n",
        "\n",
        "    # Process and save the datasets\n",
        "    for subset, data in [('train', train_dataset), ('validation', val_dataset)]:\n",
        "        if data:\n",
        "            process_and_save(data, output_folder, subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da9cec27-048a-4de8-a679-62869c8890aa",
      "metadata": {
        "id": "da9cec27-048a-4de8-a679-62869c8890aa"
      },
      "outputs": [],
      "source": [
        "# Create dataset\n",
        "output_folder = 'dataset'\n",
        "class_name = 'other'\n",
        "val_samples = 300\n",
        "save_dataset('Multimodal-Fatima/OK-VQA_train', output_folder, class_name, 'train', val_samples)\n",
        "save_dataset('Multimodal-Fatima/OK-VQA_test', output_folder, class_name, 'test')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1078bcba-0a3f-4818-a745-2f3cbdff7c3a",
      "metadata": {
        "id": "1078bcba-0a3f-4818-a745-2f3cbdff7c3a"
      },
      "source": [
        "## Install LLaVA\n",
        "\n",
        "To install the functions needed to use the model, we have to clone the original LLaVA repository and and install it in editable mode. This lets us access all functions and helper methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0387008-d4ad-4c58-bb9d-d77f71637257",
      "metadata": {
        "scrolled": true,
        "id": "a0387008-d4ad-4c58-bb9d-d77f71637257"
      },
      "outputs": [],
      "source": [
        "# The pip install -e . lets us install the repository in editable mode\n",
        "!git clone https://github.com/haotian-liu/LLaVA.git\n",
        "!cd LLaVA && pip install --upgrade pip && pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aa03b91-85a8-434a-81f6-105dfa87ecb2",
      "metadata": {
        "id": "1aa03b91-85a8-434a-81f6-105dfa87ecb2"
      },
      "source": [
        "## DeepSpeed\n",
        "\n",
        "Microsoft DeepSpeed is a deep learning optimization library designed to enhance the training speed and scalability of large-scale artificial intelligence (AI) models. Developed by Microsoft, this open-source tool specifically addresses the challenges associated with training very large models, allowing for reduced computational times and resource usage. By optimizing memory management and introducing novel parallelism techniques, DeepSpeed enables developers and researchers to train models with billions of parameters efficiently, even on limited hardware setups.DeepSpeed API is a lightweight wrapper on PyTorch. DeepSpeed manages all of the boilerplate training techniques, such as distributed training, mixed precision, gradient accumulation, and checkpoints and allows you to just focus on model development. To learn more about DeepSpeed and how it performs the magic, check out this [article](https://www.deepspeed.ai/2021/03/07/zero3-offload.html) on DeepSpeed and ZeRO.\n",
        "\n",
        "Using deepspeed is extremely simple - you simply pip install it! The LLaVA respository contains the setup scripts and configuration files needed to finetune in different ways."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55721ad3-f88f-4e03-9d99-d193c276bd0e",
      "metadata": {
        "scrolled": true,
        "id": "55721ad3-f88f-4e03-9d99-d193c276bd0e"
      },
      "outputs": [],
      "source": [
        "!cd LLaVA && pip install -e \".[train]\"\n",
        "!pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38d16e09-a3ec-461b-8a95-78c3a4e22379",
      "metadata": {
        "id": "38d16e09-a3ec-461b-8a95-78c3a4e22379"
      },
      "outputs": [],
      "source": [
        "!pip install deepspeed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "772ac37a-10ad-46b9-84f7-ef289a29bbbb",
      "metadata": {
        "id": "772ac37a-10ad-46b9-84f7-ef289a29bbbb"
      },
      "source": [
        "## Weights and Biases\n",
        "\n",
        "Weights and Biases is an industry standard MLOps tool to used to monitor and evaluate training jobs. At Brev, we use Weights and Biases to track all of our finetuning jobs! Its extremely easy to setup and plugs into the DeepSpeed training loop. You simply create an account and use the cells below to log in!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97ae96a8-f281-471c-aeb9-dac6bc7f5bb6",
      "metadata": {
        "id": "97ae96a8-f281-471c-aeb9-dac6bc7f5bb6"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "330f3d47-8daa-480a-acd7-6517bac50c9b",
      "metadata": {
        "id": "330f3d47-8daa-480a-acd7-6517bac50c9b"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fc74b4e-6c88-4e9e-92bd-610a54fa01a3",
      "metadata": {
        "id": "2fc74b4e-6c88-4e9e-92bd-610a54fa01a3"
      },
      "source": [
        "## Finetuning job\n",
        "\n",
        "Below we start the DeepSpeed training run for 5 epochs. It will automatically recognize multiple GPUs and parallelize across them. Most of the input flags are standard but you can adjust your training run with the `num_train_epochs` and `per_device_train_batch_size` flags!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2019238-2ce5-4985-98c1-7dabf9c10169",
      "metadata": {
        "id": "f2019238-2ce5-4985-98c1-7dabf9c10169"
      },
      "outputs": [],
      "source": [
        "!deepspeed LLaVA/llava/train/train_mem.py \\\n",
        "    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\n",
        "    --deepspeed LLaVA/scripts/zero3.json \\\n",
        "    --model_name_or_path liuhaotian/llava-v1.5-13b \\\n",
        "    --version v1 \\\n",
        "    --data_path ./dataset/train/dataset.json \\\n",
        "    --image_folder ./dataset/images \\\n",
        "    --vision_tower openai/clip-vit-large-patch14-336 \\\n",
        "    --mm_projector_type mlp2x_gelu \\\n",
        "    --mm_vision_select_layer -2 \\\n",
        "    --mm_use_im_start_end False \\\n",
        "    --mm_use_im_patch_token False \\\n",
        "    --image_aspect_ratio pad \\\n",
        "    --group_by_modality_length True \\\n",
        "    --bf16 True \\\n",
        "    --output_dir ./checkpoints/llava-v1.5-13b-task-lora \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --per_device_train_batch_size 16 \\\n",
        "    --per_device_eval_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --evaluation_strategy \"no\" \\\n",
        "    --save_strategy \"steps\" \\\n",
        "    --save_steps 50000 \\\n",
        "    --save_total_limit 1 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --weight_decay 0. \\\n",
        "    --warmup_ratio 0.03 \\\n",
        "    --lr_scheduler_type \"cosine\" \\\n",
        "    --logging_steps 1 \\\n",
        "    --tf32 True \\\n",
        "    --model_max_length 2048 \\\n",
        "    --gradient_checkpointing True \\\n",
        "    --dataloader_num_workers 4 \\\n",
        "    --lazy_preprocess True \\\n",
        "    --report_to wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0922ab44-5b91-4f64-9e9f-e52e54894597",
      "metadata": {
        "id": "0922ab44-5b91-4f64-9e9f-e52e54894597"
      },
      "source": [
        "Here's an excerpt from my WandB run!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e85526d8-eba3-4cfa-9354-c4906225b9b6",
      "metadata": {
        "id": "e85526d8-eba3-4cfa-9354-c4906225b9b6"
      },
      "source": [
        "![Screenshot 2024-04-17 at 11.23.31 AM.jpg](attachment:da73b948-7a81-4864-97fb-3ea3f892d67f.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "657dca5d-8014-4c28-a922-c24187d9db2e",
      "metadata": {
        "id": "657dca5d-8014-4c28-a922-c24187d9db2e"
      },
      "outputs": [],
      "source": [
        "# merge the LoRA weights with the full model\n",
        "!python LLaVA/scripts/merge_lora_weights.py --model-path checkpoints/llava-v1.5-13b-task-lora --model-base liuhaotian/llava-v1.5-13b --save-model-path llava-ftmodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e52c548-a1b4-477a-ad68-1d9ac153fd9c",
      "metadata": {
        "id": "5e52c548-a1b4-477a-ad68-1d9ac153fd9c"
      },
      "outputs": [],
      "source": [
        "# bump transformers down for gradio/deployment inference if needed\n",
        "!pip install transformers==4.37.2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff7f3d67-3c9c-4413-b86d-0a4ec2883df8",
      "metadata": {
        "id": "ff7f3d67-3c9c-4413-b86d-0a4ec2883df8"
      },
      "source": [
        "## Deployment\n",
        "\n",
        "LLaVA gives us 2 ways to deploy the model - via CLI or Gradio UI. We suggest using the Gradio UI for interactivity as you can compare two models and see the finetuning effect compared to the original model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34046faf-3e2c-4726-9e41-6f4bb3028dd6",
      "metadata": {
        "scrolled": true,
        "id": "34046faf-3e2c-4726-9e41-6f4bb3028dd6"
      },
      "outputs": [],
      "source": [
        "# Uncomment the lines below to run the CLI. You need to pass in a JPG image URL to use the multimodal capabilities\n",
        "\n",
        "# !python -m llava.serve.cli \\\n",
        "#     --model-path llava-ftmodel \\\n",
        "#     --image-file \"https://llava-vl.github.io/static/images/view.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "549a9a5a-d386-4b9b-a9f6-808217d6f9c1",
      "metadata": {
        "id": "549a9a5a-d386-4b9b-a9f6-808217d6f9c1"
      },
      "outputs": [],
      "source": [
        "# Download the model runner\n",
        "!wget -L https://raw.githubusercontent.com/brevdev/notebooks/main/assets/llava-deploy.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9550ef4-e881-49d7-a237-a997234d179e",
      "metadata": {
        "id": "a9550ef4-e881-49d7-a237-a997234d179e"
      },
      "outputs": [],
      "source": [
        "# Run inference! Use the public link provided in the output to test\n",
        "!chmod +x llava-deploy.sh && ./llava-deploy.sh"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}